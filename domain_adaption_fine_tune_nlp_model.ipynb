{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba912be-879a-4a9b-a7e9-180f42555cb9",
   "metadata": {},
   "source": [
    "## Domain Adaption: Fine-Tune Pre-Trained NLP Models\n",
    "\n",
    "### Introduction\n",
    "In today's world, the availability of pre-trained NLP models has greatly simplified the interpretation of textual data using deep learning techniques. However, while these models excel in general tasks, they often lack adaptability to specific domains. This comprehensive guide aims to walk you through the process of fine-tuning pre-trained NLP models to achieve improved performance in a particular domain.\n",
    "\n",
    "#### Motivation\n",
    "Although pre-trained NLP models like BERT and the Universal Sentence Encoder (USE) are effective in capturing linguistic intricacies, their performance in domain-specific applications can be limited due to the diverse range of datasets they are trained on. This limitation becomes evident when analyzing relationships within a specific domain. \n",
    "\n",
    "For example, when working with employment data, we expect the model to recognize the closer proximity between the roles of 'Data Scientist' and 'Machine Learning Engineer', or the stronger association between 'Python' and 'TensorFlow'. Unfortunately, general-purpose models often miss these nuanced relationships.\n",
    "\n",
    "To address this issue, we can fine-tune pre-trained models with high-quality, domain-specific datasets. This adaptation process significantly enhances the model's performance and precision, fully unlocking the potential of the NLP model.\n",
    "\n",
    "When dealing with large pre-trained NLP models, it is advisable to initially deploy the base model and consider fine-tuning only if its performance falls short for the specific problem at hand.\n",
    " \n",
    "This tutorial focuses on fine-tuning the Universal Sentence Encoder (USE) model using easily accessible open-source data.\n",
    "\n",
    "### Theoretical Overview\n",
    "Fine-tuning an ML model can be achieved through various strategies, such as supervised learning and reinforcement learning. In this tutorial, we will concentrate on a one(few)-shot learning approach combined with a siamese architecture for the fine-tuning process.\n",
    "\n",
    "#### Methodology\n",
    "In this tutorial, we utilize a siamese neural network, which is a specific type of Artificial Neural Network. This network leverages shared weights while simultaneously processing two distinct input vectors to compute comparable output vectors. Inspired by one-shot learning, this approach has proven to be particularly effective in capturing semantic similarity, although it may require longer training times and lack probabilistic output.\n",
    "\n",
    "A Siamese Neural Network creates an 'embedding space' where related concepts are positioned closely, enabling the model to better discern semantic relations.\n",
    "- Twin Branches and Shared Weights: The architecture consists of two identical branches, each containing an embedding layer with shared weights. These dual branches handle two inputs simultaneously, either similar or dissimilar.\n",
    "- Similarity and Transformation: The inputs are transformed into vector embeddings using the pre-trained NLP model. The architecture then calculates the similarity between the vectors. The similarity score, ranging between -1 and 1, quantifies the angular distance between the two vectors, serving as a metric for their semantic similarity.\n",
    "- Contrastive Loss and Learning: The model's learning is guided by the \"Contrastive Loss,\" which is the difference between the expected output (similarity score from the training data) and the computed similarity. This loss guides the adjustment of the model's weights to minimize the loss and enhance the quality of the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3c7e3-4b5f-4369-b6b0-e0ea5b43f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c770e7f-5a4e-42d7-89d6-a8eb0eec8210",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Data Overview\n",
    "\n",
    "For the fine-tuning of pre-trained NLP models using this method, the training data should consist of pairs of text strings accompanied by similarity scores between them. \n",
    "\n",
    "In this tutorial, we use a dataset sourced from the ESCO classification dataset, which has been transformed to generate similarity scores based on the relationships between different data elements.\n",
    "\n",
    "Preparing the training data is a crucial step in the fine-tuning process. It is assumed that you have access to the required data and a method to transform it into the specified format. Since the focus of this article is to demonstrate the fine-tuning process, we will omit the details of how the data was generated using the ESCO dataset.\n",
    "\n",
    "Let's start by examining the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c76e58-475b-49b4-a97e-5c7b1e6e2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data from this file is stored in the variable \"data\".\n",
    "data = pd.read_csv(\"./data/training_data.csv\")\n",
    "\n",
    "# Use the head function on the DataFrame to display its first 5 rows.\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc8c3c-2835-4472-894f-12239de8173b",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Baseline Model\n",
    "To begin, we establish the multilingual universal sentence encoder as our baseline model. It is essential to set this baseline before proceeding with the fine-tuning process.\n",
    "\n",
    "For this tutorial, we will use the STS benchmark and a sample similarity visualization as metrics to evaluate the changes and improvements achieved through the fine-tuning process.\n",
    "\n",
    "The STS Benchmark dataset consists of English sentence pairs, each associated with a similarity score. During the model training process, we evaluate the model's performance on this benchmark set. The persisted scores for each training run are the Pearson correlation between the predicted similarity scores and the actual similarity scores in the dataset. \n",
    "\n",
    "These scores ensure that as the model is fine-tuned with our context-specific training data, it maintains some level of generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643d10f-716e-4982-984e-48f0bbf2e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the Universal Sentence Encoder Multilingual module from TensorFlow Hub.\n",
    "base_model_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "base_model = tf.keras.Sequential([\n",
    "    hub.KerasLayer(base_model_url,\n",
    "                   input_shape=[],\n",
    "                   dtype=tf.string,\n",
    "                   trainable=False)\n",
    "])\n",
    "\n",
    "# Defines a list of test sentences. These sentences represent various job titles.\n",
    "test_text = ['Data Scientist', 'Data Analyst', 'Data Engineer',\n",
    "             'Nurse Practitioner', 'Registered Nurse', 'Medical Assistant',\n",
    "             'Social Media Manager', 'Marketing Strategist', 'Product Marketing Manager']\n",
    "\n",
    "# Creates embeddings for the sentences in the test_text list. \n",
    "# The np.array() function is used to convert the result into a numpy array.\n",
    "# The .tolist() function is used to convert the numpy array into a list, which might be easier to work with.\n",
    "vectors = np.array(base_model.predict(test_text)).tolist()\n",
    "\n",
    "# Calls the plot_similarity function to create a similarity plot.\n",
    "plot_similarity(test_text, vectors, 90, \"base model\")\n",
    "\n",
    "# Computes STS benchmark score for the base model\n",
    "pearsonr = sts_benchmark(base_model)\n",
    "print(\"STS Benachmark: \" + str(pearsonr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a178d6-760d-4c0b-8994-0bac82118a98",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Fine Tuning the Model\n",
    "The next step involves constructing the siamese model architecture using the baseline model and fine-tuning it with our domain-specific data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65eca14-f5c4-41f5-8048-ce7480759a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word embedding model\n",
    "embedding_layer = hub.load(base_model_url)\n",
    "\n",
    "# Create a Keras layer from the loaded embedding model\n",
    "shared_embedding_layer = hub.KerasLayer(embedding_layer, trainable=True)\n",
    "\n",
    "# Define the inputs to the model\n",
    "left_input = keras.Input(shape=(), dtype=tf.string)\n",
    "right_input = keras.Input(shape=(), dtype=tf.string)\n",
    "\n",
    "# Pass the inputs through the shared embedding layer\n",
    "embedding_left_output = shared_embedding_layer(left_input)\n",
    "embedding_right_output = shared_embedding_layer(right_input)\n",
    "\n",
    "# Compute the cosine similarity between the embedding vectors\n",
    "cosine_similarity = tf.keras.layers.Dot(axes=-1, normalize=True)(\n",
    "    [embedding_left_output, embedding_right_output]\n",
    ")\n",
    "\n",
    "# Convert the cosine similarity to angular distance\n",
    "pi = tf.constant(math.pi, dtype=tf.float32)\n",
    "clip_cosine_similarities = tf.clip_by_value(\n",
    "    cosine_similarity, -0.99999, 0.99999\n",
    ")\n",
    "acos_distance = 1.0 - (tf.acos(clip_cosine_similarities) / pi)\n",
    "\n",
    "# Package the model\n",
    "encoder = tf.keras.Model([left_input, right_input], acos_distance)\n",
    "\n",
    "# Compile the model\n",
    "encoder.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        learning_rate=0.00001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.9999,\n",
    "        epsilon=0.0000001,\n",
    "        amsgrad=False,\n",
    "        clipnorm=1.0,\n",
    "        name=\"Adam\",\n",
    "    ),\n",
    "    loss=tf.keras.losses.MeanSquaredError(\n",
    "        reduction=keras.losses.Reduction.AUTO, name=\"mean_squared_error\"\n",
    "    ),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MeanAbsoluteError(),\n",
    "        tf.keras.metrics.MeanAbsolutePercentageError(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e87af-306a-47b9-8349-ca64c54f06ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "                monitor=\"loss\", patience=3, min_delta=0.001\n",
    "            )\n",
    "logdir = os.path.join(\n",
    "                \".\",\n",
    "                \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "# Model Input\n",
    "left_inputs, right_inputs, similarity = process_model_input(data)\n",
    "\n",
    "history = encoder.fit(\n",
    "                [left_inputs, right_inputs],\n",
    "                similarity,\n",
    "                batch_size=8,\n",
    "                epochs=20,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[early_stop, tensorboard_callback],\n",
    "            )\n",
    "\n",
    "inputs = keras.Input(shape=[], dtype=tf.string)\n",
    "embedding = hub.KerasLayer(embedding_layer)(inputs)\n",
    "\n",
    "tuned_model = keras.Model(inputs=inputs, outputs=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06975f60-e3a1-4c46-b5fa-3bc2512061a6",
   "metadata": {},
   "source": [
    "** **\n",
    "#### Evaluation\n",
    "\n",
    "Now that we have the fine-tuned model, let's re-evaluate it and compare the results to those of the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3260ff05-07aa-4c55-9509-4ca60fada326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates embeddings for the sentences in the test_text list. \n",
    "# The np.array() function is used to convert the result into a numpy array.\n",
    "# The .tolist() function is used to convert the numpy array into a list, which might be easier to work with.\n",
    "vectors = np.array(tuned_model.predict(test_text)).tolist()\n",
    "\n",
    "# Calls the plot_similarity function to create a similarity plot.\n",
    "plot_similarity(test_text, vectors, 90, \"tuned model\")\n",
    "\n",
    "# Computes STS benchmark score for the tuned model\n",
    "pearsonr = sts_benchmark(tuned_model)\n",
    "print(\"STS Benachmark: \" + str(pearsonr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807399d-3360-4d7b-8413-4eb0c033e922",
   "metadata": {},
   "source": [
    "Based on fine-tuning the model on the relatively small dataset, the STS benchmark score is comparable to that of the baseline model, indicating that the tuned model still exhibits generalizability. However, the similarity visualization demonstrates strengthened similarity scores between similar titles and a reduction in scores for dissimilar ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c50ab-9d12-4c3b-8ea9-f4ca2725efd3",
   "metadata": {},
   "source": [
    "** **\n",
    "### Closing Thoughts\n",
    "\n",
    "Fine-tuning pre-trained NLP models for domain adaptation is a powerful technique to improve their performance and precision in specific contexts. By utilizing quality, domain-specific datasets and leveraging siamese neural networks, we can enhance the model's ability to capture semantic similarity.\n",
    "\n",
    "This tutorial provided a step-by-step guide to the fine-tuning process, using the Universal Sentence Encoder (USE) model as an example. We explored the theoretical framework, data preparation, baseline model evaluation, and the actual fine-tuning process. The results demonstrated the effectiveness of fine-tuning in strengthening similarity scores within a domain.\n",
    "\n",
    "By following this approach and adapting it to your specific domain, you can unlock the full potential of pre-trained NLP models and achieve better results in your natural language processing tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
